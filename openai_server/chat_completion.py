# from commons import *
# The chat completion object

from pydantic import BaseModel, Field




class FunctionCall(BaseModel):
    # 这个定义放到里面会报错，所以放到外面
    # 实际上是 ChatCompletion.Choice.Message.FunctionCall
    # 用在了ToolCall里面和FunctionCall里面
    arguments:str = Field(
        description="The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."
    ) # 这个参数是LLM生成的，不一定是合法的JSON，所以需要在调用函数之前进行验证。
    name:str = Field(
        description="The name of the function to call."
    ) # 这个函数名是LLM生成的，不一定是合法的函数名。
    

    class Message(BaseModel):
        content: str | None = Field(
            description="The contents of the message.",
            examples=['\n\nHello there, how may I assist you today?']
        )

        class ToolCall(BaseModel):
            id:str = Field(
                description="The ID of the tool call."
            )
            type:str = Field(
                description="The type of the tool. Currently, only function is supported.",
                examples=['function']
            )
            function:FunctionCall = Field(
                description="The function that the model called.",
            )
        tool_calls : list[ToolCall] = Field(None,
            description="The tool calls generated by the model, such as function calls.",
        ) # 是 Optional的，可以直接missing。 相比下面的function_call，这个是一个列表，里面是多个工具调用。
        role :str = Field(
            description="The role of the author of this message.", 
            examples = ['assistant']
        )
        function_call:FunctionCall = Field(None,
            description="Deprecated and replaced by tool_calls. The name and arguments of a function that should be called, as generated by the model."
        ) # 是 Optional的，可以直接missing
        
        
class TokenLogProb(BaseModel):
    # 这个定义放到里面会报错，所以放到外面
    # 实际上是 ChatCompletion.Choice.LogProbs.TokenLogProb
    # ChatCompletion.Choice.LogProbs.Content 包括了 自身的 TokenLogProb 其他token的 TokenLogProb
    token: str = Field(
        description="The token.",
    )
    logprob: float = Field(
        description="The log probability of this token.",
    )
    bytes: list[int] | None = Field(
        None,
        description="A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token.",
    )  # bytes只是个python内置函数，不是保留字，至少不会有语法问题，所以不需要alias。
    
    
class Content(TokenLogProb):
    top_logprobs: list[TokenLogProb] = Field(
        description="List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested top_logprobs returned.",
    )  # 在Content对象对应的token的这个位置上，其他潜在的token的logprob信息。

class LogProbs(BaseModel):

    content: list[Content] | None = Field(
        None,
        description="A list of message content tokens with log probability information.",
    )  # 这是一个列表，里面是每个token的logprob信息。


class Choice(BaseModel):
    finish_reason: str = Field(
        description="The reason the model stopped generating tokens. This will be stop if the model hit a natural stop point or a provided stop sequence, length if the maximum number of tokens specified in the request was reached, content_filter if content was omitted due to a flag from our content filters, tool_calls if the model called a tool, or function_call (deprecated) if the model called a function.",
        examples=[
            "stop",
            "length",
            "content_filter",
            "tool_calls",
            "function_call",
        ],
    )  # stop是模型自己生成了stop token。length是超过了预设的max_tokens。content_filter是因为内容被过滤了。tool_calls是因为调用了工具。function_call是因为调用了函数。
    index: int = Field(
        description="The index of the choice in the list of choices.", examples=[0]
    )  # 应该是前面的选择更好？或者仅仅是LLM生成的先后顺序？具体好坏是在下面的logprobs里面。

    message: Message = Field(
        description="A chat completion message generated by the model.",
    )  # 这是核心部分

    logprobs: LogProbs | None = Field(
        None,
        description="Log probability information for the choice.",
    )
         
class Usage(BaseModel):
    completion_tokens: int = Field(
        description="Number of tokens in the generated completion.", examples=[9]
    )
    prompt_tokens: int = Field(
        description="Number of tokens in the prompt.", examples=[12]
    )
    total_tokens: int = Field(
        description="Total number of tokens used in the request (prompt + completion).",
        examples=[21],
    )



class ChatCompletion(BaseModel):
    """Represents a chat completion response returned by model, based on the provided input.
    :py:class: Usage
    :py:class: Choice
    """


    # 也就是一会儿我们服务器要返回的LLM回复。
    id: str = Field(
        description="A unique identifier for the chat completion.",
        examples=["chatcmpl-123"],
    )

    choices: list[Choice] = Field(
        description="A list of chat completion choices. Can be more than one if n is greater than 1.",
    )  # 其实意思就是一次AI请求可以有多个备选回复。
    created: int = Field(
        description="The Unix timestamp (in seconds) of when the chat completion was created.",
        examples=[1677652288],
    )
    model: str = Field(
        description="The model used for the chat completion.",
        examples=["gpt-3.5-turbo-0613"],
    )
    system_fingerprint: str = Field(None,
        description="""This fingerprint represents the backend configuration that the model runs with.
Can be used in conjunction with the seed request parameter to understand when backend changes have been made that might impact determinism.""",
        examples=["fp_44709d6fcb"],
    )
    object: str = Field(
        description="The object type, which is always chat.completion.",
        examples=["chat.completion"],
    )

    usage: Usage = Field(
        description="Usage statistics for the completion request.",
    )
    
class ChatCompletionChunk(BaseModel):
    id: str
    object: str
    created: int
    model: str
    system_fingerprint: str
    choices: list[dict]
